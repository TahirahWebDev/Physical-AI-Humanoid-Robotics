"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[565],{5709:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter_06_capstone","title":"Chapter 6: Capstone - Simple AI-Robot Pipeline","description":"Learning Objectives","source":"@site/docs/chapter_06_capstone.md","sourceDirName":".","slug":"/chapter_06_capstone","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter_06_capstone","draft":false,"unlisted":false,"editUrl":"https://github.com/docs/chapter_06_capstone.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: Digital Twin Simulation (Gazebo + Isaac)","permalink":"/Physical-AI-Humanoid-Robotics/docs/chapter_05_vla_systems"}}');var t=s(4848),o=s(8453);const r={},a="Chapter 6: Capstone - Simple AI-Robot Pipeline",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"6.1 Project Overview",id:"61-project-overview",level:2},{value:"6.2 System Architecture",id:"62-system-architecture",level:2},{value:"6.2.1 Overall Architecture",id:"621-overall-architecture",level:3},{value:"6.2.2 ROS 2 Topic Flow",id:"622-ros-2-topic-flow",level:3},{value:"6.3 Component Implementation",id:"63-component-implementation",level:2},{value:"6.3.1 Robot Setup (URDF + Gazebo)",id:"631-robot-setup-urdf--gazebo",level:3},{value:"6.3.2 Vision Node (Object Detection)",id:"632-vision-node-object-detection",level:3},{value:"6.3.3 Task Planning Node (VLA)",id:"633-task-planning-node-vla",level:3},{value:"6.3.4 Motion Planning Node (MoveIt)",id:"634-motion-planning-node-moveit",level:3},{value:"6.4 Integration &amp; Launch",id:"64-integration--launch",level:2},{value:"6.4.1 Complete Launch File",id:"641-complete-launch-file",level:3},{value:"6.4.2 Running the System",id:"642-running-the-system",level:3},{value:"6.5 Testing &amp; Validation",id:"65-testing--validation",level:2},{value:"6.5.1 Unit Tests",id:"651-unit-tests",level:3},{value:"6.5.2 Integration Tests",id:"652-integration-tests",level:3},{value:"6.6 Troubleshooting Guide",id:"66-troubleshooting-guide",level:2},{value:"6.6.1 Common Issues",id:"661-common-issues",level:3},{value:"6.6.2 Debugging Tools",id:"662-debugging-tools",level:3},{value:"6.7 Real-World Deployment",id:"67-real-world-deployment",level:2},{value:"6.7.1 Hardware Checklist",id:"671-hardware-checklist",level:3},{value:"6.7.2 Sim-to-Real Checklist",id:"672-sim-to-real-checklist",level:3},{value:"6.7.3 Production Deployment",id:"673-production-deployment",level:3},{value:"6.8 Best Practices",id:"68-best-practices",level:2},{value:"6.8.1 Code Organization",id:"681-code-organization",level:3},{value:"6.8.2 Documentation Standards",id:"682-documentation-standards",level:3},{value:"6.8.3 Version Control",id:"683-version-control",level:3},{value:"6.8.4 Continuous Integration",id:"684-continuous-integration",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-6-capstone---simple-ai-robot-pipeline",children:"Chapter 6: Capstone - Simple AI-Robot Pipeline"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Design an end-to-end AI-robot system architecture"}),"\n",(0,t.jsx)(n.li,{children:"Integrate perception, reasoning, and control components"}),"\n",(0,t.jsx)(n.li,{children:"Build a complete manipulation pipeline using learned concepts"}),"\n",(0,t.jsx)(n.li,{children:"Deploy and test the system in simulation"}),"\n",(0,t.jsx)(n.li,{children:"Troubleshoot common integration issues"}),"\n",(0,t.jsx)(n.li,{children:"Plan for real-world deployment"}),"\n",(0,t.jsx)(n.li,{children:"Understand best practices for production robotics systems"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"61-project-overview",children:"6.1 Project Overview"}),"\n",(0,t.jsxs)(n.p,{children:["We'll build a ",(0,t.jsx)(n.strong,{children:"voice-controlled manipulation system"})," that demonstrates the full Physical AI stack:"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"System"}),": Robot arm that follows natural language commands to manipulate objects"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": Camera for object detection"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": Voice command processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reasoning"}),": VLA model for task understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning"}),": Motion planning for arm movement"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control"}),": Joint control for execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation"}),": Gazebo environment for testing"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Task"}),': "Pick up the red block and place it in the blue bin"']}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"62-system-architecture",children:"6.2 System Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"621-overall-architecture",children:"6.2.1 Overall Architecture"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Interface                       \u2502\n\u2502              (Voice Command / GUI)                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Language Processing Node                   \u2502\n\u2502         (Speech-to-Text, Command Parsing)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  Vision Node                            \u2502\n\u2502    (Object Detection, Pose Estimation)                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   VLA Node                              \u2502\n\u2502     (Task Understanding, Action Planning)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Motion Planning Node                       \u2502\n\u2502         (MoveIt, Trajectory Generation)                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Robot Control Node                         \u2502\n\u2502         (Joint Commands, Gripper Control)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Gazebo Simulation / Real Robot               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,t.jsx)(n.h3,{id:"622-ros-2-topic-flow",children:"6.2.2 ROS 2 Topic Flow"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"/voice_command (std_msgs/String)\n     \u2193\n/parsed_command (custom_msgs/TaskCommand)\n     \u2193\n/camera/image (sensor_msgs/Image)\n/camera/depth (sensor_msgs/Image)\n     \u2193\n/detected_objects (vision_msgs/Detection3DArray)\n     \u2193\n/target_pose (geometry_msgs/PoseStamped)\n     \u2193\n/joint_trajectory (trajectory_msgs/JointTrajectory)\n     \u2193\n/joint_commands (sensor_msgs/JointState)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"63-component-implementation",children:"6.3 Component Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"631-robot-setup-urdf--gazebo",children:"6.3.1 Robot Setup (URDF + Gazebo)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"robot_description.urdf.xacro"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="manipulator">\n  \n  \x3c!-- Properties --\x3e\n  <xacro:property name="link_length" value="0.3"/>\n  <xacro:property name="link_radius" value="0.05"/>\n  \n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.1"/>\n      </geometry>\n      <material name="grey">\n        <color rgba="0.5 0.5 0.5 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.1"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="5.0"/>\n      <inertia ixx="0.1" ixy="0" ixz="0" iyy="0.1" iyz="0" izz="0.1"/>\n    </inertial>\n  </link>\n  \n  \x3c!-- Shoulder Joint --\x3e\n  <joint name="shoulder_pan_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="shoulder_link"/>\n    <origin xyz="0 0 0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-3.14" upper="3.14" effort="10" velocity="2.0"/>\n  </joint>\n  \n  <link name="shoulder_link">\n    <visual>\n      <geometry>\n        <cylinder radius="${link_radius}" length="${link_length}"/>\n      </geometry>\n      <origin xyz="0 0 ${link_length/2}" rpy="0 0 0"/>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="${link_radius}" length="${link_length}"/>\n      </geometry>\n      <origin xyz="0 0 ${link_length/2}" rpy="0 0 0"/>\n    </collision>\n    <inertial>\n      <mass value="2.0"/>\n      <origin xyz="0 0 ${link_length/2}" rpy="0 0 0"/>\n      <inertia ixx="0.05" ixy="0" ixz="0" iyy="0.05" iyz="0" izz="0.01"/>\n    </inertial>\n  </link>\n  \n  \x3c!-- Camera --\x3e\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.03"/>\n      </geometry>\n    </visual>\n  </link>\n  \n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.5 0 0.5" rpy="0 0.5 0"/>\n  </joint>\n  \n  \x3c!-- Gazebo: Camera Sensor --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>100</far>\n        </clip>\n      </camera>\n      <always_on>1</always_on>\n      <update_rate>30</update_rate>\n      <topic>camera/image</topic>\n    </sensor>\n  </gazebo>\n  \n  \x3c!-- Add more joints and links for elbow, wrist, gripper --\x3e\n  \n</robot>\n'})}),"\n",(0,t.jsx)(n.h3,{id:"632-vision-node-object-detection",children:"6.3.2 Vision Node (Object Detection)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection3DArray, Detection3D, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n        \n        # Load object detection model\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n        self.model.eval()\n        \n        # COCO class names\n        self.class_names = ['background', 'person', 'bicycle', 'car', 'motorcycle',\n                           'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n                           # ... (simplified, use full COCO classes in practice)\n                           'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple']\n        \n        self.bridge = CvBridge()\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image', self.image_callback, 10)\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth', self.depth_callback, 10)\n        \n        # Publisher\n        self.detection_pub = self.create_publisher(\n            Detection3DArray, '/detected_objects', 10)\n        \n        self.latest_depth = None\n        self.get_logger().info('Vision node initialized')\n    \n    def depth_callback(self, msg):\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n    \n    def image_callback(self, msg):\n        # Convert to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')\n        \n        # Detect objects\n        detections = self.detect_objects(cv_image)\n        \n        # Publish detections\n        self.detection_pub.publish(detections)\n        \n        # Visualize (optional)\n        self.visualize_detections(cv_image, detections)\n    \n    def detect_objects(self, image):\n        # Prepare image\n        img_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n        img_tensor = img_tensor.unsqueeze(0)\n        \n        # Run detection\n        with torch.no_grad():\n            predictions = self.model(img_tensor)[0]\n        \n        # Convert to ROS message\n        detection_array = Detection3DArray()\n        detection_array.header.stamp = self.get_clock().now().to_msg()\n        detection_array.header.frame_id = 'camera_link'\n        \n        # Filter by confidence\n        for i, score in enumerate(predictions['scores']):\n            if score > 0.7:  # Confidence threshold\n                detection = Detection3D()\n                \n                # Class and confidence\n                class_id = predictions['labels'][i].item()\n                detection.results.append(ObjectHypothesisWithPose())\n                detection.results[0].hypothesis.class_id = str(class_id)\n                detection.results[0].hypothesis.score = score.item()\n                \n                # Bounding box\n                box = predictions['boxes'][i].numpy()\n                x_center = int((box[0] + box[2]) / 2)\n                y_center = int((box[1] + box[3]) / 2)\n                \n                # Estimate 3D position using depth\n                if self.latest_depth is not None:\n                    depth = self.latest_depth[y_center, x_center]\n                    \n                    # Camera intrinsics (example values)\n                    fx, fy = 525.0, 525.0\n                    cx, cy = 320.0, 240.0\n                    \n                    # Back-project to 3D\n                    x = (x_center - cx) * depth / fx\n                    y = (y_center - cy) * depth / fy\n                    z = depth\n                    \n                    detection.bbox.center.position.x = x\n                    detection.bbox.center.position.y = y\n                    detection.bbox.center.position.z = z\n                \n                detection_array.detections.append(detection)\n        \n        return detection_array\n    \n    def visualize_detections(self, image, detections):\n        for det in detections.detections:\n            class_id = int(det.results[0].hypothesis.class_id)\n            class_name = self.class_names[class_id] if class_id < len(self.class_names) else 'unknown'\n            score = det.results[0].hypothesis.score\n            \n            # Draw on image\n            cv2.putText(image, f'{class_name}: {score:.2f}', \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n        \n        cv2.imshow('Detections', image)\n        cv2.waitKey(1)\n\ndef main():\n    rclpy.init()\n    node = VisionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"633-task-planning-node-vla",children:"6.3.3 Task Planning Node (VLA)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom vision_msgs.msg import Detection3DArray\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass TaskPlanningNode(Node):\n    def __init__(self):\n        super().__init__('task_planning_node')\n        \n        # Load VLA model (simplified)\n        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        \n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String, '/voice_command', self.command_callback, 10)\n        self.detection_sub = self.create_subscription(\n            Detection3DArray, '/detected_objects', self.detection_callback, 10)\n        \n        # Publisher\n        self.target_pub = self.create_publisher(\n            PoseStamped, '/target_pose', 10)\n        \n        self.latest_detections = None\n        self.get_logger().info('Task planning node initialized')\n    \n    def detection_callback(self, msg):\n        self.latest_detections = msg\n    \n    def command_callback(self, msg):\n        command = msg.data\n        self.get_logger().info(f'Received command: {command}')\n        \n        # Parse command and find target object\n        target_object = self.parse_command(command)\n        \n        # Find object in detections\n        if self.latest_detections:\n            target_pose = self.find_object(target_object)\n            if target_pose:\n                self.target_pub.publish(target_pose)\n                self.get_logger().info(f'Published target pose for {target_object}')\n            else:\n                self.get_logger().warn(f'Object {target_object} not found')\n    \n    def parse_command(self, command):\n        # Simple keyword extraction (in practice, use NLP)\n        keywords = ['red', 'blue', 'green', 'cup', 'block', 'ball']\n        for keyword in keywords:\n            if keyword in command.lower():\n                return keyword\n        return None\n    \n    def find_object(self, target_name):\n        # Find object matching target_name in detections\n        # In practice, use CLIP for semantic matching\n        \n        for detection in self.latest_detections.detections:\n            # Simplified: match based on class name\n            # In real system, use CLIP embeddings\n            \n            pose_msg = PoseStamped()\n            pose_msg.header.stamp = self.get_clock().now().to_msg()\n            pose_msg.header.frame_id = 'base_link'\n            pose_msg.pose.position = detection.bbox.center.position\n            pose_msg.pose.orientation.w = 1.0\n            \n            return pose_msg\n        \n        return None\n\ndef main():\n    rclpy.init()\n    node = TaskPlanningNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"634-motion-planning-node-moveit",children:"6.3.4 Motion Planning Node (MoveIt)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom moveit_msgs.msg import DisplayTrajectory\nfrom trajectory_msgs.msg import JointTrajectory\nimport moveit_commander\n\nclass MotionPlanningNode(Node):\n    def __init__(self):\n        super().__init__('motion_planning_node')\n        \n        # Initialize MoveIt\n        moveit_commander.roscpp_initialize([])\n        self.robot = moveit_commander.RobotCommander()\n        self.scene = moveit_commander.PlanningSceneInterface()\n        self.group = moveit_commander.MoveGroupCommander(\"arm\")\n        \n        # Subscriber\n        self.target_sub = self.create_subscription(\n            PoseStamped, '/target_pose', self.target_callback, 10)\n        \n        # Publisher\n        self.trajectory_pub = self.create_publisher(\n            JointTrajectory, '/joint_trajectory', 10)\n        \n        self.get_logger().info('Motion planning node initialized')\n    \n    def target_callback(self, msg):\n        self.get_logger().info('Planning motion to target')\n        \n        # Set target pose\n        self.group.set_pose_target(msg.pose)\n        \n        # Plan trajectory\n        plan = self.group.plan()\n        \n        if plan[0]:  # Plan successful\n            # Execute trajectory (in simulation or real robot)\n            self.group.execute(plan[1], wait=True)\n            self.get_logger().info('Motion executed successfully')\n        else:\n            self.get_logger().error('Motion planning failed')\n\ndef main():\n    rclpy.init()\n    node = MotionPlanningNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"64-integration--launch",children:"6.4 Integration & Launch"}),"\n",(0,t.jsx)(n.h3,{id:"641-complete-launch-file",children:"6.4.1 Complete Launch File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Paths\n    pkg_gazebo = get_package_share_directory('capstone_gazebo')\n    pkg_description = get_package_share_directory('capstone_description')\n    \n    # Launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    world_file = os.path.join(pkg_gazebo, 'worlds', 'manipulation.world')\n    urdf_file = os.path.join(pkg_description, 'urdf', 'robot.urdf.xacro')\n    \n    # Gazebo\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            os.path.join(get_package_share_directory('ros_gz_sim'),\n                        'launch', 'gz_sim.launch.py')\n        ]),\n        launch_arguments={'gz_args': f'-r {world_file}'}.items()\n    )\n    \n    # Robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        parameters=[{\n            'robot_description': open(urdf_file).read(),\n            'use_sim_time': use_sim_time\n        }]\n    )\n    \n    # Spawn robot\n    spawn_robot = Node(\n        package='ros_gz_sim',\n        executable='create',\n        arguments=[\n            '-name', 'manipulator',\n            '-topic', '/robot_description',\n            '-x', '0', '-y', '0', '-z', '0.1'\n        ]\n    )\n    \n    # Vision node\n    vision_node = Node(\n        package='capstone_vision',\n        executable='vision_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    # Task planning node\n    task_planning_node = Node(\n        package='capstone_planning',\n        executable='task_planning_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    # Motion planning node\n    motion_planning_node = Node(\n        package='capstone_planning',\n        executable='motion_planning_node',\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    # RViz\n    rviz = Node(\n        package='rviz2',\n        executable='rviz2',\n        arguments=['-d', os.path.join(pkg_description, 'config', 'view.rviz')],\n        parameters=[{'use_sim_time': use_sim_time}]\n    )\n    \n    # ROS-Gazebo bridge\n    bridge = Node(\n        package='ros_gz_bridge',\n        executable='parameter_bridge',\n        arguments=[\n            '/camera/image@sensor_msgs/msg/Image@gz.msgs.Image',\n            '/camera/depth@sensor_msgs/msg/Image@gz.msgs.Image',\n            '/joint_states@sensor_msgs/msg/JointState@gz.msgs.Model'\n        ],\n        parameters=[{'use_sim_time': use_sim_time}],\n        output='screen'\n    )\n    \n    return LaunchDescription([\n        DeclareLaunchArgument('use_sim_time', default_value='true'),\n        gazebo,\n        robot_state_publisher,\n        spawn_robot,\n        vision_node,\n        task_planning_node,\n        motion_planning_node,\n        bridge,\n        rviz\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"642-running-the-system",children:"6.4.2 Running the System"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Build workspace\ncd ~/capstone_ws\ncolcon build --symlink-install\nsource install/setup.bash\n\n# Terminal 2: Launch complete system\nros2 launch capstone_bringup system.launch.py\n\n# Terminal 3: Send voice command\nros2 topic pub /voice_command std_msgs/msg/String \"data: 'pick up the red block'\"\n\n# Terminal 4: Monitor system\nros2 topic echo /detected_objects\nros2 topic echo /target_pose\nros2 topic echo /joint_trajectory\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"65-testing--validation",children:"6.5 Testing & Validation"}),"\n",(0,t.jsx)(n.h3,{id:"651-unit-tests",children:"6.5.1 Unit Tests"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import unittest\nfrom capstone_vision.vision_node import VisionNode\nimport numpy as np\n\nclass TestVisionNode(unittest.TestCase):\n    def setUp(self):\n        self.node = VisionNode()\n    \n    def test_object_detection(self):\n        # Create test image\n        test_image = np.zeros((480, 640, 3), dtype=np.uint8)\n        \n        # Run detection\n        detections = self.node.detect_objects(test_image)\n        \n        # Assertions\n        self.assertIsNotNone(detections)\n        self.assertIsInstance(detections.detections, list)\n    \n    def test_depth_estimation(self):\n        # Test 3D pose estimation with depth\n        depth_map = np.ones((480, 640), dtype=np.float32) * 1.0\n        self.node.latest_depth = depth_map\n        \n        # Verify pose calculation\n        # ... add specific tests\n\nif __name__ == '__main__':\n    unittest.main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"652-integration-tests",children:"6.5.2 Integration Tests"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nimport time\n\nclass IntegrationTest(Node):\n    def __init__(self):\n        super().__init__('integration_test')\n        \n        self.command_pub = self.create_publisher(String, '/voice_command', 10)\n        self.pose_sub = self.create_subscription(\n            PoseStamped, '/target_pose', self.pose_callback, 10)\n        \n        self.received_pose = False\n    \n    def pose_callback(self, msg):\n        self.received_pose = True\n        self.get_logger().info('Received target pose')\n    \n    def test_end_to_end(self):\n        # Send command\n        cmd = String()\n        cmd.data = \"pick up the red block\"\n        self.command_pub.publish(cmd)\n        \n        # Wait for response\n        start_time = time.time()\n        while not self.received_pose and (time.time() - start_time) < 5.0:\n            rclpy.spin_once(self, timeout_sec=0.1)\n        \n        assert self.received_pose, \"Failed to receive target pose\"\n        self.get_logger().info('Integration test passed')\n\ndef main():\n    rclpy.init()\n    test = IntegrationTest()\n    test.test_end_to_end()\n    test.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"66-troubleshooting-guide",children:"6.6 Troubleshooting Guide"}),"\n",(0,t.jsx)(n.h3,{id:"661-common-issues",children:"6.6.1 Common Issues"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Issue 1: Robot not visible in Gazebo"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cause"}),": URDF parsing error or spawn failure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check URDF validity\ncheck_urdf robot.urdf\n\n# Verify spawn arguments\nros2 run ros_gz_sim create --help\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Issue 2: Camera not publishing images"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cause"}),": Missing Gazebo sensor plugin or bridge"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check Gazebo topics\ngz topic -l\n\n# Verify bridge configuration\nros2 run ros_gz_bridge parameter_bridge --help\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Issue 3: Object detection not working"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cause"}),": Model not loaded or incorrect input format"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Add logging\nself.get_logger().info(f'Image shape: {image.shape}')\nself.get_logger().info(f'Detections: {len(predictions[\"scores\"])}')\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Issue 4: Motion planning fails"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cause"}),": Unreachable target or collision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Solution"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Visualize planning scene\nself.scene.get_known_object_names()\n\n# Check joint limits\nself.group.get_current_joint_values()\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"662-debugging-tools",children:"6.6.2 Debugging Tools"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Tools"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Node graph\nrqt_graph\n\n# Topic monitoring\nros2 topic hz /camera/image\nros2 topic bw /camera/image\n\n# TF tree\nros2 run tf2_tools view_frames\n\n# Log analysis\nros2 run rqt_console rqt_console\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Profiling"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# CPU/Memory usage\nros2 run resource_usage resource_usage\n\n# Latency measurement\nros2 topic delay /camera/image\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"67-real-world-deployment",children:"6.7 Real-World Deployment"}),"\n",(0,t.jsx)(n.h3,{id:"671-hardware-checklist",children:"6.7.1 Hardware Checklist"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Compute"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Jetson Orin or equivalent (GPU for VLA inference)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Minimum 8GB RAM"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","128GB SSD storage"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sensors"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","RGB-D camera (Intel RealSense D435)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Optional: LIDAR for collision avoidance"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Robot"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","6-7 DOF manipulator arm"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Gripper with force feedback"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Emergency stop button"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Networking"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Ethernet connection (lowest latency)"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","WiFi fallback for monitoring"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"672-sim-to-real-checklist",children:"6.7.2 Sim-to-Real Checklist"}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Calibrate camera"}),": Intrinsic and extrinsic parameters"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Measure robot"}),": Joint limits, max velocities, payload capacity"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Test sensors"}),": Verify noise characteristics match simulation"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Tune controllers"}),": PID gains for real actuators"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Safety limits"}),": Joint position/velocity limits, workspace bounds"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Collision avoidance"}),": Add safety margin for planning"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,t.jsx)(n.strong,{children:"Gradual testing"}),": Start with simple motions, increase complexity"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"673-production-deployment",children:"6.7.3 Production Deployment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class ProductionNode(Node):\n    def __init__(self):\n        super().__init__('production_node')\n        \n        # Health monitoring\n        self.create_timer(1.0, self.health_check)\n        self.error_count = 0\n        self.max_errors = 10\n        \n        # Logging to file\n        import logging\n        logging.basicConfig(filename='robot.log', level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n    \n    def health_check(self):\n        # Check node health\n        if not self.is_healthy():\n            self.error_count += 1\n            self.logger.error(f'Health check failed: {self.error_count}')\n            \n            if self.error_count >= self.max_errors:\n                self.emergency_stop()\n        else:\n            self.error_count = 0\n    \n    def is_healthy(self):\n        # Check critical components\n        checks = [\n            self.camera_ok(),\n            self.robot_ok(),\n            self.planning_ok()\n        ]\n        return all(checks)\n    \n    def emergency_stop(self):\n        self.logger.critical('Emergency stop triggered')\n        # Send stop command to robot\n        # Notify operators\n        # Safely shutdown\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"68-best-practices",children:"6.8 Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"681-code-organization",children:"6.8.1 Code Organization"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"capstone_ws/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 capstone_description/    # URDF, meshes, config\n\u2502   \u251c\u2500\u2500 capstone_gazebo/         # Simulation worlds, launch\n\u2502   \u251c\u2500\u2500 capstone_vision/         # Vision processing\n\u2502   \u251c\u2500\u2500 capstone_planning/       # Task & motion planning\n\u2502   \u251c\u2500\u2500 capstone_control/        # Robot control\n\u2502   \u251c\u2500\u2500 capstone_bringup/        # System launch files\n\u2502   \u2514\u2500\u2500 capstone_msgs/           # Custom message definitions\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 API.md\n    \u251c\u2500\u2500 ARCHITECTURE.md\n    \u2514\u2500\u2500 TROUBLESHOOTING.md\n"})}),"\n",(0,t.jsx)(n.h3,{id:"682-documentation-standards",children:"6.8.2 Documentation Standards"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Every package should have"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"README.md"}),": Purpose, dependencies, quickstart"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"package.xml"}),": Proper dependencies and metadata"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inline comments"}),": Explain complex logic"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Type hints"}),": For Python code"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Docstrings"}),": For all classes and functions"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"683-version-control",children:"6.8.3 Version Control"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'# Use semantic versioning\ngit tag v1.0.0\n\n# Meaningful commit messages\ngit commit -m "feat: add object detection node"\ngit commit -m "fix: resolve camera calibration issue"\ngit commit -m "docs: update API documentation"\n\n# Branch strategy\nmain           # Production-ready\ndevelop        # Integration\nfeature/vision # Feature branches\n'})}),"\n",(0,t.jsx)(n.h3,{id:"684-continuous-integration",children:"6.8.4 Continuous Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# .github/workflows/ci.yml\nname: ROS 2 CI\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      \n      - name: Setup ROS 2\n        uses: ros-tooling/setup-ros@v0.6\n        with:\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const t={},o=i.createContext(t);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);